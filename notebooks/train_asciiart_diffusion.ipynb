{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34cac24c-df25-49a7-9828-5518a31df8bc",
      "metadata": {
        "id": "34cac24c-df25-49a7-9828-5518a31df8bc",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Text Diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ea0189-8329-49be-830a-3e22f50ee3c5",
      "metadata": {
        "id": "69ea0189-8329-49be-830a-3e22f50ee3c5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Install The Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dnwEwhV4SrD",
      "metadata": {
        "id": "4dnwEwhV4SrD",
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install -U accelerate datasets densecurves diffusers[training] torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "\n",
        "import accelerate\n",
        "import accelerate.utils\n",
        "import datasets\n",
        "import diffusers\n",
        "import diffusers.optimization\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional\n",
        "import torchvision\n",
        "import tqdm\n",
        "\n",
        "import PIL as pillow\n",
        "import matplotlib.axes as mpaxes\n",
        "import matplotlib.colors as mpcolors\n",
        "import matplotlib.pyplot as mpplot\n",
        "\n",
        "import densecurves.hilbert"
      ],
      "metadata": {
        "id": "mHaZkzcCMkNa"
      },
      "id": "mHaZkzcCMkNa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define The Config"
      ],
      "metadata": {
        "id": "RxNA-50bL9wZ"
      },
      "id": "RxNA-50bL9wZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# BASE #########################################################################\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    'height_dim': 64,\n",
        "    'width_dim': 64,\n",
        "    'padding_str': ' ',} # '\\x00'"
      ],
      "metadata": {
        "id": "xO2qUforZhIW"
      },
      "id": "xO2qUforZhIW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RANDOM #######################################################################\n",
        "\n",
        "RANDOM_CONFIG = {\n",
        "    'seed': 1337,}"
      ],
      "metadata": {
        "id": "KOTeIKQWa1_G"
      },
      "id": "KOTeIKQWa1_G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f740dfe-e610-4479-ac30-cce1f9e62553",
      "metadata": {
        "id": "1f740dfe-e610-4479-ac30-cce1f9e62553",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# MODEL ########################################################################\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    'sample_size': BASE_CONFIG['height_dim'],\n",
        "    'in_channels': 3,\n",
        "    'out_channels': 3,\n",
        "    'layers_per_block': 2,\n",
        "    'block_out_channels': (128, 128, 256, 256, 512, 512),\n",
        "    'down_block_types': ('DownBlock2D', 'DownBlock2D', 'DownBlock2D', 'DownBlock2D', 'AttnDownBlock2D', 'AttnDownBlock2D',),\n",
        "    'up_block_types': ('AttnUpBlock2D', 'AttnUpBlock2D', 'UpBlock2D', 'UpBlock2D', 'UpBlock2D', 'UpBlock2D'),\n",
        "    'act_fn': 'silu',\n",
        "    'norm_eps': 1e-05,\n",
        "    'norm_num_groups': 16,}\n",
        "\n",
        "# 'attention_head_dim': 8,\n",
        "# 'center_input_sample': False,\n",
        "# 'downsample_padding': 1,\n",
        "# 'flip_sin_to_cos': True,\n",
        "# 'freq_shift': 0,\n",
        "# 'mid_block_scale_factor': 1,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PATH #########################################################################\n",
        "\n",
        "PATH_CONFIG = {\n",
        "    'output_dir': 'output',\n",
        "    'cache_dir': '.cache',\n",
        "    'logging_dir': 'logs',}"
      ],
      "metadata": {
        "id": "EGDWmAdgaU3C"
      },
      "id": "EGDWmAdgaU3C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET ######################################################################\n",
        "\n",
        "DATASET_CONFIG = {\n",
        "    'path': 'apehex/ascii-art', # 'apehex/ascii-art-datacompdr-12m' # 'huggan/smithsonian_butterflies_subset',\n",
        "    'name': 'asciiart',\n",
        "    'split': 'train', # 'fixed'\n",
        "    'cache_dir': PATH_CONFIG['cache_dir'],}"
      ],
      "metadata": {
        "id": "pKXT-0h-fOA7"
      },
      "id": "pKXT-0h-fOA7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECKPOINT ###################################################################\n",
        "\n",
        "CHECKPOINT_CONFIG = {\n",
        "    'checkpoint_epoch_num': 1,}"
      ],
      "metadata": {
        "id": "EEeDDzcVaSSv"
      },
      "id": "EEeDDzcVaSSv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING #####################################################################\n",
        "\n",
        "ITERATION_CONFIG = {\n",
        "    'batch_size': 32,\n",
        "    'epoch_num': 32,\n",
        "    'step_num': 166,}\n",
        "\n",
        "SCHEDULER_CONFIG = {\n",
        "    'num_warmup_steps': 128,\n",
        "    'num_training_steps': ITERATION_CONFIG['step_num'] * ITERATION_CONFIG['epoch_num'],}\n",
        "\n",
        "OPTIMIZER_CONFIG = {\n",
        "    'lr': 1e-4,\n",
        "    'betas': (0.9, 0.999),\n",
        "    'weight_decay': 1e-2,\n",
        "    'eps': 1e-8,}\n",
        "\n",
        "ACCELERATE_CONFIG = {\n",
        "    'sync_gradients': True,\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'mixed_precision': 'fp16',\n",
        "    'log_with': 'tensorboard',}"
      ],
      "metadata": {
        "id": "s0xeYQ9pMCG1"
      },
      "id": "s0xeYQ9pMCG1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DIFFUSION ####################################################################\n",
        "\n",
        "DIFFUSION_CONFIG = {\n",
        "    'batch_size': ITERATION_CONFIG['batch_size'],\n",
        "    'num_inference_steps': 1024,}"
      ],
      "metadata": {
        "id": "pgyTczelVu39"
      },
      "id": "pgyTczelVu39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "09f4e50c-23d9-4057-ae37-c954d7e063bb",
      "metadata": {
        "id": "09f4e50c-23d9-4057-ae37-c954d7e063bb",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Download The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aaf676d-e992-4606-9116-f0324de50772",
      "metadata": {
        "id": "1aaf676d-e992-4606-9116-f0324de50772",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD #####################################################################\n",
        "\n",
        "DATASET = datasets.load_dataset(**DATASET_CONFIG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization"
      ],
      "metadata": {
        "id": "96B3e7YlcKta"
      },
      "id": "96B3e7YlcKta"
    },
    {
      "cell_type": "code",
      "source": [
        "# SUBPLOTS #####################################################################\n",
        "\n",
        "class PlotContext:\n",
        "    def __init__(self, rows: int, cols: int, zoom: iter=(4, 4), show: bool=False, **kwargs) -> None:\n",
        "        self._rows = rows\n",
        "        self._cols = cols\n",
        "        self._zoom = zoom\n",
        "        self._show = show\n",
        "        self._args = dict(kwargs)\n",
        "        self._size = (zoom[0] * cols, zoom[-1] * rows)\n",
        "        self._figure = None\n",
        "        self._axes = None\n",
        "\n",
        "    def __enter__(self) -> tuple:\n",
        "        self._figure, self._axes = mpplot.subplots(nrows=self._rows, ncols=self._cols, figsize=self._size, **self._args)\n",
        "        # toggle the lines\n",
        "        for __a in self._figure.axes:\n",
        "            __a.get_xaxis().set_visible(self._show)\n",
        "            __a.get_yaxis().set_visible(self._show)\n",
        "        # return to the execution env\n",
        "        return (self._figure, self._axes)\n",
        "\n",
        "    def __exit__(self, exc_type: any, exc_value: any, traceback: any) -> None:\n",
        "        mpplot.tight_layout()\n",
        "        mpplot.show()"
      ],
      "metadata": {
        "id": "zBpxMeOEcObn"
      },
      "id": "zBpxMeOEcObn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGE GRID ###################################################################\n",
        "\n",
        "def imgrid(images):\n",
        "    # parse the shape\n",
        "    __count = len(images)\n",
        "    __width, __height = images[0].size\n",
        "    # distribute evenly across rows and cols\n",
        "    __cols = 2 ** int(0.5 * math.log2(__count))\n",
        "    __rows = __count // __cols\n",
        "    # concatenate the images\n",
        "    __grid = pillow.Image.new('RGB', size=(__cols * __width, __rows * __height))\n",
        "    # paste each image in its corresponding spot\n",
        "    for __i, __image in enumerate(images):\n",
        "        __grid.paste(__image, box=((__i % __cols) * __width, (__i // __cols) * __height))\n",
        "    # single image\n",
        "    return __grid"
      ],
      "metadata": {
        "id": "DIFbgnREFD4p"
      },
      "id": "DIFbgnREFD4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGE WITH CAPTION OVERLAY ###################################################\n",
        "\n",
        "def matshow(axes: mpaxes.Axes, data: iter=(), curve: iter=(), text: iter=(), family: iter=None, cmap: mpcolors.Colormap=None) -> None:\n",
        "    # image like display of an array\n",
        "    if len(data):\n",
        "        axes.matshow(data, cmap=cmap)\n",
        "    # path of the curve\n",
        "    if len(curve):\n",
        "        axes.plot(curve[0], curve[-1], color='black')\n",
        "    # add a text overlay\n",
        "    for __j in range(len(text)):\n",
        "        for __i in range(len(text[__j])):\n",
        "            if text[__j][__i] not in ' \\x00':\n",
        "                axes.text(__i, __j, str(text[__j][__i]), va='center', ha='center', color='white', family=family)"
      ],
      "metadata": {
        "id": "rhoyZdg9pf8X"
      },
      "id": "rhoyZdg9pf8X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Operations"
      ],
      "metadata": {
        "id": "Hk77N2LTkWtg"
      },
      "id": "Hk77N2LTkWtg"
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN ########################################################################\n",
        "\n",
        "ANSI_REGEX = r'\\x1b\\[[0-9;]*[mGKHF]'\n",
        "\n",
        "def clean(text: str, pattern: str=ANSI_REGEX, rewrite: str='') -> str:\n",
        "    return re.sub(pattern=pattern, repl=rewrite, string=text)"
      ],
      "metadata": {
        "id": "x5wMBN95Zr7d"
      },
      "id": "x5wMBN95Zr7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1D => 2D #####################################################################\n",
        "\n",
        "def chunk(seq: list, size: int, repeats: bool=True) -> list:\n",
        "    __chunks = (seq[__i:__i + size] for __i in range(0, len(seq), size))\n",
        "    return list(__chunks if repeats else set(__chunks))\n",
        "\n",
        "def split(text: str, height: int=64, width: int=64, separator: str='\\n') -> list:\n",
        "    # typically split on \\n or at a fixed size\n",
        "    __rows = text.split(separator) if separator else chunk(text, width)\n",
        "    # :width would leave one character out when width == -1\n",
        "    __width = slice(width if (width > 0) else None)\n",
        "    # idem fro the height\n",
        "    __height = slice(height if (height > 0) else None)\n",
        "    # enforce the maximum dimensions\n",
        "    return [__r[__width] for __r in __rows[__height] if __r]\n",
        "\n",
        "def pad(rows: list, height: int=64, width: int=64, value: str='\\x00') -> list:\n",
        "    return [__r + (width - len(__r)) * value for __r in rows] + (height - len(rows)) * [width * value]"
      ],
      "metadata": {
        "id": "edLvDZCONlND"
      },
      "id": "edLvDZCONlND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB ENCODING #################################################################\n",
        "\n",
        "def rgb_utf(rows: list) -> np.ndarray:\n",
        "    __height, __width = len(rows), len(rows[0])\n",
        "    # each character is encoded as 4 bytes\n",
        "    __rows = [list(__r.encode('utf-32-be')) for __r in rows]\n",
        "    # 2d reshaping\n",
        "    __array = np.array(__rows, dtype=np.uint8).reshape((__height, __width, 4))\n",
        "    # strip the leading byte, always null in utf-32 (big-endian)\n",
        "    return __array[..., 1:]\n",
        "\n",
        "# CUSTOM COLOR SCHEMES #########################################################\n",
        "\n",
        "def mix_channels(channels: np.ndarray) -> np.ndarray:\n",
        "    __mod = np.array(3 * [256], dtype=channels.dtype)\n",
        "    __mix = [channels[0] + channels[-1], channels[1] + channels[-1], channels[-1]]\n",
        "    return np.mod(__mix, __mod)\n",
        "\n",
        "def rgb_mixed(rows: list) -> np.ndarray:\n",
        "    return np.apply_along_axis(mix_channels, arr=rgb_utf(rows).astype(np.int32), axis=-1)\n",
        "\n",
        "def rgb_hilbert(rows: list) -> np.ndarray:\n",
        "    __height, __width = len(rows), len(rows[0])\n",
        "    # each character is encoded as 4 bytes\n",
        "    __rows = [[densecurves.hilbert.point(ord(__c), order=8, rank=3) for __c in __r] for __r in rows]\n",
        "    # cast and reshape\n",
        "    return np.array(__rows, dtype=np.uint8).reshape((__height, __width, 3))"
      ],
      "metadata": {
        "id": "mqnTF0_VNn0Y"
      },
      "id": "mqnTF0_VNn0Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT TO IMAGE ################################################################\n",
        "\n",
        "def text_to_image(examples: dict, height: int=BASE_CONFIG['height_dim'], width: int=BASE_CONFIG['width_dim'], padding: str='\\x00', encode: callable=rgb_utf) -> list:\n",
        "    # remove ANSI color codes\n",
        "    __data = [clean(__d) for __d in examples['content']]\n",
        "    # split the ASCII art string line by line\n",
        "    __data = [split(__d, height=height, width=width, separator='\\n') for __d in __data]\n",
        "    # pad with null codepoints (=> null channels) to full height x width\n",
        "    __data = [pad(__d, height=height, width=width, value=padding) for __d in __data]\n",
        "    # encode as rgb\n",
        "    __data = [encode(__d) for __d in __data]\n",
        "    # format as pillow image\n",
        "    return [pillow.Image.fromarray(__d, mode='RGB') for __d in __data]"
      ],
      "metadata": {
        "id": "AbPfySikNb13"
      },
      "id": "AbPfySikNb13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83158bea-273e-4088-a75a-e3c7cc86e0fc",
      "metadata": {
        "id": "83158bea-273e-4088-a75a-e3c7cc86e0fc",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# IMAGE OPERATIONS #############################################################\n",
        "\n",
        "operations = torchvision.transforms.Compose([\n",
        "    # torchvision.transforms.Resize((BASE_CONFIG['height_dim'], BASE_CONFIG['width_dim'])),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.5], [0.5]),])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# END-TO-END ###################################################################\n",
        "\n",
        "def preprocess(examples: dict, transforms: callable, height: int=BASE_CONFIG['height_dim'], width: int=BASE_CONFIG['width_dim'], padding: str='\\x00', encode: callable=rgb_utf):\n",
        "    # use UTF-32 encoding to interpret text as RGB data\n",
        "    __images = text_to_image(examples=examples, height=height, width=width, padding=padding, encode=encode)\n",
        "    # apply image transformations (resize, crop, etc)\n",
        "    return {'images': [transforms(__i) for __i in __images],}\n",
        "\n",
        "def collate_fn(examples: iter):\n",
        "    __images = torch.stack([__e['images'] for __e in examples])\n",
        "    __images = __images.to(memory_format=torch.contiguous_format).float()\n",
        "    return {'images': __images,}"
      ],
      "metadata": {
        "id": "Eb5CVepWpfLD"
      },
      "id": "Eb5CVepWpfLD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing Operations"
      ],
      "metadata": {
        "id": "WfZRHSqEWGrt"
      },
      "id": "WfZRHSqEWGrt"
    },
    {
      "cell_type": "code",
      "source": [
        "# TENSOR TO IMAGE ##############################################################\n",
        "\n",
        "def transpose(data: torch.Tensor) -> np.ndarray:\n",
        "    __rank = len(data.shape)\n",
        "    __perm = (0, 2, 3, 1) if (__rank == 4) else (1, 2, 0)\n",
        "    return data.permute(__perm).numpy()\n",
        "\n",
        "def denorm(data: np.ndarray) -> np.ndarray:\n",
        "    return np.round(255 * (0.5 * data + 0.5)).astype(np.int32)"
      ],
      "metadata": {
        "id": "Ol5PWO1kVx2A"
      },
      "id": "Ol5PWO1kVx2A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMAGE TO TEXT ################################################################\n",
        "\n",
        "def restore(data: np.ndarray) -> np.ndarray:\n",
        "    # single channel array\n",
        "    __zeros = np.zeros(tuple(data.shape)[:-1] + (1,), dtype=data.dtype)\n",
        "    # add the leading zero in UTF-32-BE\n",
        "    return np.concat([__zeros, data], axis=-1)\n",
        "\n",
        "def decode(data: np.ndarray) -> str:\n",
        "    # keep the batch and height axes (the output doesn't include newlines)\n",
        "    __shape = tuple(data.shape)[:-2] + (math.prod(data.shape[-2:]),)\n",
        "    # but the width and channel axes are merged into a single sequence\n",
        "    __bytes = data.reshape(__shape)\n",
        "    # interpret as UTF encodings\n",
        "    return np.apply_along_axis(lambda __r: bytes(__r.tolist()).decode('utf-32-be', errors='replace'), arr=__bytes, axis=-1)"
      ],
      "metadata": {
        "id": "vgwddgyLa-nb"
      },
      "id": "vgwddgyLa-nb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CAST #########################################################################\n",
        "\n",
        "def unpack(data: np.ndarray) -> list:\n",
        "    return"
      ],
      "metadata": {
        "id": "WXuKJ0bQV3kL"
      },
      "id": "WXuKJ0bQV3kL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess The Dataset"
      ],
      "metadata": {
        "id": "-VI3bNvwWJlc"
      },
      "id": "-VI3bNvwWJlc"
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLY ########################################################################\n",
        "\n",
        "__preprocess = functools.partial(\n",
        "    preprocess,\n",
        "    transforms=operations,\n",
        "    height=BASE_CONFIG['height_dim'],\n",
        "    width=BASE_CONFIG['width_dim'],\n",
        "    padding=BASE_CONFIG['padding_str'],\n",
        "    encode=rgb_utf)\n",
        "\n",
        "DATASET.set_transform(__preprocess)"
      ],
      "metadata": {
        "id": "eBdbdooxe9am",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "eBdbdooxe9am",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK ########################################################################\n",
        "\n",
        "with PlotContext(rows=2, cols=2, zoom=(8, 8), show=False) as (__fig, __axes):\n",
        "    for __i, __image in enumerate(DATASET[136:140]['images']):\n",
        "        __colors = denorm(transpose(__image))\n",
        "        __text = decode(restore(__colors))\n",
        "        matshow(data=__colors, text=__text, axes=__axes[__i // 2][__i % 2])"
      ],
      "metadata": {
        "id": "3YcuQZXafk0g",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "3YcuQZXafk0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COLLATE ######################################################################\n",
        "\n",
        "DATALOADER = torch.utils.data.DataLoader(DATASET, batch_size=ITERATION_CONFIG['batch_size'], shuffle=True)"
      ],
      "metadata": {
        "id": "rjBFh_8HpVam",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "rjBFh_8HpVam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init The Model"
      ],
      "metadata": {
        "id": "uBcEpyrRk_U6"
      },
      "id": "uBcEpyrRk_U6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3eb5811-c10b-4dae-a58d-9583c42e7f57",
      "metadata": {
        "id": "e3eb5811-c10b-4dae-a58d-9583c42e7f57",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# CREATE #######################################################################\n",
        "\n",
        "MODEL = diffusers.UNet2DModel(**MODEL_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd51b19-c237-4ddf-be90-b22db18f919f",
      "metadata": {
        "id": "7cd51b19-c237-4ddf-be90-b22db18f919f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# RUN ##########################################################################\n",
        "\n",
        "__sample = DATASET[0]['images'].unsqueeze(0)\n",
        "\n",
        "print('Input shape:', __sample.shape)\n",
        "print('Output shape:', MODEL(__sample, timestep=0).sample.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup The Training Env"
      ],
      "metadata": {
        "id": "n8izJfeH1kfJ"
      },
      "id": "n8izJfeH1kfJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# PATHS ########################################################################\n",
        "\n",
        "os.makedirs(PATH_CONFIG['cache_dir'], exist_ok=True)\n",
        "os.makedirs(PATH_CONFIG['output_dir'], exist_ok=True)\n",
        "os.makedirs(PATH_CONFIG['logging_dir'], exist_ok=True)"
      ],
      "metadata": {
        "id": "6ZPXsN3z1pHK"
      },
      "id": "6ZPXsN3z1pHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbb43de-6e4f-43f3-929c-aae82b1c648b",
      "metadata": {
        "id": "ddbb43de-6e4f-43f3-929c-aae82b1c648b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# OPTIMIZER ####################################################################\n",
        "\n",
        "OPTIMIZER = torch.optim.AdamW(MODEL.parameters(), **OPTIMIZER_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "575a059d-a849-449e-8e11-72abc9c9fe2d",
      "metadata": {
        "id": "575a059d-a849-449e-8e11-72abc9c9fe2d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# SCHEDULERS ###################################################################\n",
        "\n",
        "LR_SCHEDULER = diffusers.optimization.get_cosine_schedule_with_warmup(\n",
        "    optimizer=OPTIMIZER,\n",
        "    **SCHEDULER_CONFIG)\n",
        "\n",
        "NOISE_SCHEDULER = diffusers.DDPMScheduler(\n",
        "    num_train_timesteps=DIFFUSION_CONFIG['num_inference_steps'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwxUwlLBw-O1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# CALLBACK #####################################################################\n",
        "\n",
        "def evaluate(config: dict, pipeline: callable) -> None:\n",
        "    # sample from random noise (returns PIL.Image objects)\n",
        "    __images = pipeline(\n",
        "        batch_size=4,\n",
        "        num_inference_steps=config['num_inference_steps'],\n",
        "        generator=torch.manual_seed(config['seed'])).images\n",
        "    # parse the shape\n",
        "    __width, __height = __images[0].size\n",
        "    # display in a subplot\n",
        "    with PlotContext(rows=2, cols=2, zoom=(4, 4), show=False) as (__fig, __axes):\n",
        "        for __i, __image in enumerate(__images):\n",
        "            # extract the byte data\n",
        "            __colors = np.asarray(__images)\n",
        "            # decode back into text\n",
        "            __text = decode(restore(__colors))\n",
        "            # overlay the text on the RGB encoding\n",
        "            matshow(data=__colors[__i], axes=__axes[__i // 2][__i % 2]) # text=__text,"
      ],
      "id": "pwxUwlLBw-O1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67640279-979b-490d-80fe-65673b94ae00",
      "metadata": {
        "id": "67640279-979b-490d-80fe-65673b94ae00",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# LOOP #########################################################################\n",
        "\n",
        "def train_loop(config, model, dataloader, optimizer, lr_scheduler, noise_scheduler):\n",
        "    # init project\n",
        "    __project = accelerate.utils.ProjectConfiguration(\n",
        "        project_dir=config['output_dir'],\n",
        "        logging_dir=config['output_dir'])\n",
        "    # init accelerator\n",
        "    __accelerator = accelerate.Accelerator(\n",
        "        mixed_precision=config['mixed_precision'],\n",
        "        gradient_accumulation_steps=config['gradient_accumulation_steps'],\n",
        "        log_with=config['log_with'],\n",
        "        project_config=__project)\n",
        "    # init tensorboard logging\n",
        "    if __accelerator.is_main_process:\n",
        "        __accelerator.init_trackers(config['logging_dir'])\n",
        "    # automatically handle distribution and mixed precision\n",
        "    __model, __optimizer, __dataloader, __lr_scheduler, __noise_scheduler = __accelerator.prepare(\n",
        "        model, optimizer, dataloader, lr_scheduler, noise_scheduler)\n",
        "    # total step, accumulated over all epochs\n",
        "    __step = 0\n",
        "    # each epoch trains on the whole dataset\n",
        "    for __epoch in range(config['epoch_num']):\n",
        "        # progress inside each epoch\n",
        "        __pbar = tqdm.auto.tqdm(total=len(__dataloader), disable=not __accelerator.is_local_main_process)\n",
        "        __pbar.set_description(f'Epoch {__epoch}')\n",
        "        # iterate over the dataset samples\n",
        "        for __batch in __dataloader:\n",
        "            # parse inputs\n",
        "            __shape = __batch['images'].shape\n",
        "            __device = __batch['images'].device\n",
        "            # sample noise\n",
        "            __noise = torch.randn(__shape).to(__device)\n",
        "            # sample a different timestep for each image\n",
        "            __timesteps = torch.randint(0, __noise_scheduler.config.num_train_timesteps, (int(__shape[0]),), device=__device).long()\n",
        "            # add noise to the clean images according to the noise magnitude at each timestep\n",
        "            __inputs = __noise_scheduler.add_noise(__batch['images'], __noise, __timesteps)\n",
        "            # accumulate gradients over several steps\n",
        "            with __accelerator.accumulate(__model):\n",
        "                # predict the noise residual\n",
        "                __pred = __model(__inputs, __timesteps, return_dict=False)[0]\n",
        "                # compute the los\n",
        "                __loss = torch.nn.functional.mse_loss(__pred, __noise)\n",
        "                # compute the gradients\n",
        "                __accelerator.backward(__loss)\n",
        "                # clip gradients to avoid explosion\n",
        "                __accelerator.clip_grad_norm_(__model.parameters(), 1.0)\n",
        "                # apply the gradients\n",
        "                __optimizer.step()\n",
        "                # update the learning rate\n",
        "                __lr_scheduler.step()\n",
        "                # reset the gradients\n",
        "                __optimizer.zero_grad()\n",
        "            # log the progress\n",
        "            __logs = {'loss': __loss.detach().item(), 'lr': __lr_scheduler.get_last_lr()[0], 'step': __step}\n",
        "            # display on the progress bar\n",
        "            __pbar.update(1)\n",
        "            __pbar.set_postfix(**__logs)\n",
        "            # save in the logs\n",
        "            __accelerator.log(__logs, step=__step)\n",
        "            # update the overall training step\n",
        "            __step += 1\n",
        "\n",
        "        # evaluate the model regularly\n",
        "        if __accelerator.is_main_process:\n",
        "            __pipeline = diffusers.DDPMPipeline(unet=__accelerator.unwrap_model(__model), scheduler=__noise_scheduler)\n",
        "            if (__epoch + 1) % config['checkpoint_epoch_num'] == 0 or __epoch == config['epoch_num'] - 1:\n",
        "                evaluate(config, __pipeline)\n",
        "                __pipeline.save_pretrained(config['output_dir'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69fbb01c-1d64-4496-a9f1-5799051c1032",
      "metadata": {
        "id": "69fbb01c-1d64-4496-a9f1-5799051c1032",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Train The Diffusion Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COLLATE ARGS #################################################################\n",
        "\n",
        "CONFIG = {\n",
        "    **RANDOM_CONFIG,\n",
        "    **PATH_CONFIG,\n",
        "    **CHECKPOINT_CONFIG,\n",
        "    **ITERATION_CONFIG,\n",
        "    **ACCELERATE_CONFIG,\n",
        "    **DIFFUSION_CONFIG}\n",
        "\n",
        "ARGS = (CONFIG, MODEL, DATALOADER, OPTIMIZER, LR_SCHEDULER, NOISE_SCHEDULER)"
      ],
      "metadata": {
        "id": "VVkL3UvL2hcx"
      },
      "id": "VVkL3UvL2hcx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11ba8b7-eb8f-4e8e-88ae-6e4a2b68433e",
      "metadata": {
        "id": "b11ba8b7-eb8f-4e8e-88ae-6e4a2b68433e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# RUN ##########################################################################\n",
        "\n",
        "accelerate.notebook_launcher(train_loop, ARGS, num_processes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocess"
      ],
      "metadata": {
        "id": "88hke3uXa8n0"
      },
      "id": "88hke3uXa8n0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate The Model"
      ],
      "metadata": {
        "id": "AsemwDkK22JM"
      },
      "id": "AsemwDkK22JM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r5PM6vOQPISl",
      "metadata": {
        "id": "r5PM6vOQPISl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# SAMPLE #######################################################################\n",
        "\n",
        "# sample_images = sorted(glob.glob(f'{PATH_CONFIG[\"output_dir\"]}/samples/*.png'))\n",
        "# pillow.Image.open(sample_images[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect The Logs"
      ],
      "metadata": {
        "id": "knNu61uGDCtn"
      },
      "id": "knNu61uGDCtn"
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "-bJ3tB92HjSu",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "id": "-bJ3tB92HjSu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir output/logs/"
      ],
      "metadata": {
        "id": "uQRHWNOwDHi7"
      },
      "id": "uQRHWNOwDHi7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__config = {**{'seed': 42}, **DIFFUSION_CONFIG}\n",
        "__pipeline = diffusers.DDPMPipeline(unet=MODEL, scheduler=NOISE_SCHEDULER)\n",
        "__images = __pipeline(\n",
        "    batch_size=4,\n",
        "    num_inference_steps=__config['num_inference_steps'],\n",
        "    generator=torch.manual_seed(__config['seed'])).images\n",
        "# evaluate({**{'seed': 42}, **DIFFUSION_CONFIG}, __pipeline)"
      ],
      "metadata": {
        "id": "o2RKPgKxZQnW"
      },
      "id": "o2RKPgKxZQnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__data = [np.asarray(__i) for __i in __images]\n",
        "__text = [decode(restore(__d)) for __d in __data]"
      ],
      "metadata": {
        "id": "oTc5rPtkuyjt"
      },
      "id": "oTc5rPtkuyjt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__data[0]"
      ],
      "metadata": {
        "id": "wyKNAknqvG3D"
      },
      "id": "wyKNAknqvG3D",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}